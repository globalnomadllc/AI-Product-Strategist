---
layout: post
title: "AI Bias: How It Happens, Why It Matters, and Ways to Fix It"
author: alex_thorpe
categories: 
  - Artificial Intelligence
  - Ethics
  - Machine Learning
image: assets/images/asmo.jpeg
tags: 
  - AI Bias
  - Fairness in AI
  - Machine Learning Ethics
  - Bias Mitigation
  - Technology
  - Data Science
---

Artificial intelligence (AI) can seem magical. It powers search engines, recommends movies, and even helps doctors. But there's a **big challenge** many people don’t see: **bias**. AI bias can happen when a computer program treats some groups unfairly. This blog post looks at why AI bias happens, how we can fix it, and even shows some **sample code** based on a well-known research paper.

---

## **What Is AI Bias?**

Imagine you’re using a robot helper. You teach it by giving it examples of real people who got hired at a company. If those examples were mostly men, the robot might learn that men get hired more. Then it could start picking men for job interviews—leaving out equally qualified women. That’s AI bias.

**Key Points:**
- Bias happens because AI learns from **historical data** (which might be unfair).
- Bias can also come from **badly designed** or **incomplete** data sets.
- If we don’t check for bias, **real people** can be harmed in important decisions (like lending, hiring, and healthcare).

---

## **Why Should We Care?**

1. **Fairness and Equality:** It’s wrong to treat people unfairly based on race, gender, or age.  
2. **Legal Requirements:** Laws like **Title VII** (in the U.S.) or the **GDPR** (in the EU) try to prevent discrimination.  
3. **Better Outcomes:** Reducing bias can make AI more **accurate** for everyone.

---

## **How Do We Fix AI Bias?**

A paper titled  
[“Bias Mitigation Methods: Applicability, Legality, and Recommendations for Development”](https://jair.org/index.php/jair/article/view/16759/27113)  
by Suresh and Guttag (published in *Journal of Artificial Intelligence Research*) offers **three broad ways** to fix (or *mitigate*) bias:

1. **Pre-Processing:**  
   - **Before** training a model, clean or rebalance your data. You might give extra weight to underrepresented groups so the model learns about them properly.

2. **In-Processing:**  
   - **While** training a model, add fairness rules to the learning process. For instance, you can include a *“fairness penalty”* in your loss function. If the model becomes too biased, the penalty grows.

3. **Post-Processing:**  
   - **After** you train the model, adjust its final predictions. You might tweak decision thresholds separately for each demographic group so the model doesn’t favor one group over another.

> **Quote from the Paper (Section 3, p. 7):**  
> “We group bias mitigation techniques into three main categories, each operating on a different slice of the machine learning pipeline.”

---

## **Digging Deeper into the Problem**

Bias can show up **anywhere** in an AI pipeline. Data might be skewed if it was gathered in ways that **exclude** certain populations. Or, the learning algorithm might “reward” patterns that happen to benefit one group. The paper stresses that bias isn’t just a technical glitch—it’s also a **legal** and **ethical** issue.

**Laws and Ethics:**  
- In some places, you can’t even store **protected attributes** (like race or religion) for model training. But if you avoid these attributes completely, you might **miss** fixing bias.  
- The authors warn that applying certain bias fixes could violate anti-discrimination laws if you’re altering protected group data directly.

---

## **Code Example: How to Use the Research**

Below is a **simplified** Python example inspired by the in-processing approach described in the paper. It shows how to add a **fairness penalty** to a logistic regression model. A part of the code checks *demographic parity*, which looks at whether different groups receive the “positive” outcome at equal rates.

```python
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# 1. Create Fake Data
N = 200
np.random.seed(42)

# Suppose half our samples belong to a sensitive group = 1, others = 0
sensitive_attr = np.random.binomial(1, 0.5, N)
X = np.random.randn(N, 3) + sensitive_attr[:, None]  # Shift distribution
y = (X[:, 0] + X[:, 1] + 0.5*sensitive_attr + np.random.randn(N)*0.5 > 0).astype(int)

X_train, X_test, y_train, y_test, s_train, s_test = train_test_split(
    X, y, sensitive_attr, test_size=0.3, random_state=42
)

# 2. Standard Logistic Regression
clf = LogisticRegression(solver='liblinear')
clf.fit(X_train, y_train)
y_pred_standard = clf.predict(X_test)

acc_standard = accuracy_score(y_test, y_pred_standard)

# Demographic Parity: difference in positive outcome rates between groups
def demographic_parity_difference(y_pred, sensitive):
    group0_mean = np.mean(y_pred[sensitive == 0])
    group1_mean = np.mean(y_pred[sensitive == 1])
    return abs(group0_mean - group1_mean)

dp_diff_standard = demographic_parity_difference(y_pred_standard, s_test)
print("Standard Model Accuracy:", acc_standard)
print("Standard Model Demographic Parity Difference:", dp_diff_standard)

# 3. In-Processing: Add a Fairness Penalty
weights = np.zeros(X_train.shape[1])
bias = 0.0
lr = 0.01
lambda_fair = 0.5  # weight for fairness

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

for epoch in range(200):
    z = np.dot(X_train, weights) + bias
    preds = sigmoid(z)
    error = preds - y_train
    
    # Standard logistic regression gradients
    grad_w = np.dot(X_train.T, error) / len(X_train)
    grad_b = np.mean(error)
    
    # Demographic Parity "penalty"
    preds_bin = (preds >= 0.5).astype(int)
    dp_diff = demographic_parity_difference(preds_bin, s_train)
    
    # This is a naive approximation for demonstration
    # We'll treat dp_diff as a simple penalty
    # Update weights with fairness penalty
    weights -= lr * (grad_w + lambda_fair * dp_diff)
    bias -= lr * (grad_b + lambda_fair * dp_diff)

# Evaluate fairness-penalized model
z_test = np.dot(X_test, weights) + bias
test_preds = (sigmoid(z_test) >= 0.5).astype(int)

acc_fair = accuracy_score(y_test, test_preds)
dp_diff_fair = demographic_parity_difference(test_preds, s_test)
print("Fairness-Penalized Model Accuracy:", acc_fair)
print("Fairness-Penalized Model DP Difference:", dp_diff_fair)
```

### **What’s Happening in the Code?**
1. **We Make a Fake Dataset**: Half the people belong to a “sensitive group.”  
2. **We Train a Standard Model**: We measure accuracy and how different the outcome rates are for each group.  
3. **We Apply a Fairness Penalty**: On each training step, we **subtract** a small amount from our weights if the demographic parity difference is too large. This tries to **even** out the treatment for both groups.

**Note**: This example is very simplified. Real systems often use specialized libraries like [Fairlearn](https://fairlearn.org/) or [AIF360](https://aif360.mybluemix.net/) to handle more complex fairness definitions (like **equalized odds** or **predictive rate parity**).

---

## **Extra Details from the Paper**

1. **Trade-Offs**: The paper warns that if you push too hard for fairness, you might lower your model’s accuracy. This trade-off can be okay if it helps your system treat all groups more **fairly**.  
2. **Legal Angle**: If laws say you can’t collect certain sensitive information, you must think carefully about how to fix bias. Some methods need knowledge of protected attributes.  
3. **No One-Size-Fits-All**: The paper suggests trying multiple methods, picking the one best suited to your data and your **ethical** or **legal** constraints.

> **Quote (Section 5, p. 18):**  
> “Combining pre-processing and in-processing strategies often yields better fairness across multiple metrics, though it may affect overall model performance.”

---

## **More In-Depth on the Problem**

- **Historical Inequities**: Often, our past actions weren’t fair, and AI learns from that past. If lenders gave fewer loans to certain groups for **decades**, an AI might copy those patterns.  
- **Complex Data**: Sometimes, the difference isn’t a single “bad” column. Instead, a combination of variables might hint at someone’s race or gender. You might think you removed “race,” but the model can still infer it from a zip code or job title.  
- **Lack of Standards**: People argue about what “fair” even means. Should each group have equal results, or should we only focus on **equal chance** of correct predictions?

---

## **Putting the Research Into Practice**

1. **Audit Your Data**: Look for missing or skewed info.  
2. **Pick Your Fairness Goal**: Do you want each group to get the same chance at a positive outcome? Or do you want each group to have equal error rates?  
3. **Try Different Mitigation Techniques**: Pre-processing, in-processing, and post-processing each solve bias from different angles.  
4. **Check the Laws**: Make sure your approach is legal where you operate.  
5. **Monitor Over Time**: A model can become biased again as conditions change. Keep an eye on fairness metrics regularly.

---

## **Other Great Resources**

- [**Algorithmic Justice League**](https://www.ajlunited.org/): Shows real stories of AI bias in daily life.  
- [**AI Now Institute**](https://ainowinstitute.org/): Research on AI’s impact on law, society, and ethics.  
- [**Fairlearn**](https://fairlearn.org/) & [**AIF360**](https://aif360.mybluemix.net/): Tools for measuring bias, building fair models, and checking results.

---

## **Final Thoughts**

AI bias may seem tough to beat, but it’s not impossible. By **cleaning data**, **adding fairness rules**, and **checking your results** at every step, you can build models that work better for everyone. This work takes time and care. It’s not just about programming. It’s about people and their **real-life opportunities**.

If you use the steps from the research paper—**pre-processing**, **in-processing**, and **post-processing**—you’ll be well on your way to a fairer AI. Remember: no single method fixes everything. Keep learning, keep testing, and keep talking to different people about what “fairness” really means in your context.

> **A Note for Developers**: Experiment with the code above in a local environment. See how changing the **lambda_fair** value adjusts the bias and accuracy. Watch how shifting the dataset affects the model. This is how you truly grasp the “why” behind bias mitigation.
